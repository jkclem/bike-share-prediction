---
title: "Analysis"
author: "John Clements and Jingjing Li"
date: "7/1/2021"
output: html_document
  toc: TRUE
#params:  weekday
---

# Introduction

We are using the daily [bike sharing dataset](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset) to demonstrate how to use machine learning for predictive modeling. The data set contains daily rentals for a bike share service, information on the weather, date information, and indicators for if the day was a holiday, working day, or a weekday (and that is an inclusive **or**).

We believe that the weather will be predictive of the number of bike rentals because few people want to bike in poor weather conditions. We also believe there will be seasonal effects because weather is related to the seasons. We also suspect there may be relationships between whether a day is a working day or holiday. There may be more demand on working days as people commute to work, or there may be more demand on holidays when people want to go about town.

```{r message=FALSE, warning=FALSE}
library(readr)
library(tidyverse)
library(cowplot)
library(caret)
library(rmarkdown)
```

# Data

Before doing any analysis, we need to read in the daily bike-share data and subset for the day of interest.

```{r, message=FALSE}
# Read in the daily data set.
bikeDay <- read_csv("./Bike-Sharing-Dataset/day.csv")
# Filter for only the day of interest.
bikeDay <- filter(bikeDay, weekday==0)
```

Some of the variables, namely, `season`, `mnth`, `holiday`, `workingday`, and `weathersit` are categorical, but are stored numerically. Before proceeding with
the analysis, we are converting them to `factor` data types.

```{r}
# Convert categorical variables stored numerically to factors.
bikeDay <- bikeDay %>%
  mutate(season = factor(ifelse(season == 1, "Spring",     
                                ifelse(season == 2, "Summer", 
                                       ifelse(season == 3, "Fall",
                                              "Winter"))),
                         levels=c("Spring", "Summer", "Fall", "Winter")),
         mnth = factor(mnth, levels=1:12),
         holiday = factor(holiday, levels=c(0, 1)),
         workingday = factor(workingday, levels=c(0, 1)),
         weathersit = ordered(weathersit, levels=c(1, 2, 3, 4))
         )
```

With that done, we can split the two data sets into training and testing sets for predictive modeling.

```{r}
# Set a random seed for reproducibility.
set.seed(10)

# Set the proportion of observations to use for training models.
trainProp <- 0.7

# Get indexes for the training and testing subsets of the daily data.
trainIndexes <- sample(1:nrow(bikeDay),
                          size=nrow(bikeDay)*trainProp)
testIndexes <- setdiff(1:nrow(bikeDay), trainIndexes)
# Use the indexes to separate the training and testing sets of the daily data.
bikeTrain <- bikeDay[trainIndexes, ]
bikeTest <- bikeDay[testIndexes, ]
```

With our data sets created, we can now begin exploring the data.

# Summarizations

Let's begin our exploration with the numeric variables related to weather: temperature, humidity, and wind speed. Below are the means and standard deviations for each of these variables by season.

```{r echo=FALSE}
# Find the means and standard deviations of temperature, humidity and wind speed
# by season.
seasonalSummary <- bikeTrain %>% 
  group_by(season) %>% 
  summarise(
    avgtemp = mean(temp*41), sdtemp = sd(temp*41), 
    avghum = mean(hum*100), sdhum = sd(hum*100), 
    avgwindspd = mean(windspeed*67), sdwindspd=sd(windspeed*67)
    )


knitr::kable(
  seasonalSummary,
  col.names=c("Season", 
              "Temp. Mean", "Temp. Std. Dev.",
              "Humidity Mean", "Humidity Std. Dev.",
              "Wind Speed Mean", "Wind Speed Std. Dev."),
  digits=2,
  caption=paste0("Table 1: Means and Standard Deviations for Daily Temp. ",
                 "(Celsius), Humidity, and Wind Speed (km/h)")
)
```

The box plots below displays the quartiles and any outliers graphically.

```{r echo=FALSE}
###
# Create box plots of the variables mentioned above, grouped by season, with
# jittered dots to display actual values.
###

plot1 <- ggplot(bikeTrain, aes(x=season, y=temp*41, color=season)) + 
  geom_boxplot() +
  geom_jitter(alpha=0.5) +
  labs(x="Season",
       y="Temp. (Celsius)",
       title="Temp. by Season") + 
  theme(legend.position="none")

plot2 <- ggplot(bikeTrain, aes(x=season, y=hum*100, color=season)) + 
  geom_boxplot() +
  geom_jitter(alpha=0.5) +
  labs(x="Season",
       y="Humidity",
       title="Humidity by Season") + 
  theme(legend.position="none")

plot3 <- ggplot(bikeTrain, aes(x=season, y=windspeed*67, 
                                        color=season)) + 
  geom_boxplot() +
  geom_jitter(alpha=0.5) +
  labs(x="Season",
       y="Wind Speed (km/h)",
       title="Wind Speed by Season") + 
  theme(legend.position="none")

plot_grid(plot1, plot2, plot3)
```

Now that we've seen how temperature, humidity, and wind speed relate to the seasons, let's look at how they relate to each other.

```{r echo=FALSE}
###
# Create scatter plots between temp, hum, and windspeed with regression lines.
###

plot4 <- ggplot(bikeTrain, aes(temp*41, hum*100)) + 
  geom_point(color="blue", alpha=0.5) + 
  geom_smooth(method = 'lm', formula='y ~ poly(x, 2)', color="red") +
  xlab("Temp. (Celsius)") + 
  ylab("Humidity") + 
  ggtitle("Humidity vs. Temp.")

plot5 <- ggplot(bikeTrain, aes(temp*41, windspeed*67)) + 
  geom_point(color="blue", alpha=0.5) + 
  geom_smooth(method = 'lm', formula='y ~ x', color="red") +
  xlab("Temp. (Celsius)") + 
  ylab("Wind Speed (km/h)") + 
  ggtitle("Wind Speed vs. Temp.")

plot6 <- ggplot(bikeTrain, aes(hum*100, windspeed*67)) + 
  geom_point(color="blue", alpha=0.5) + 
  geom_smooth(method = 'lm', formula='y ~ poly(x, 2)', color="red") +
  xlab("Humidity") + 
  ylab("Wind Speed (km/h)") + 
  ggtitle("Wind Speed vs. Humidity")

plot_grid(plot4, plot5, plot6)
```

Now that we have examined the relationship between the seasons and our numeric measures of the weather, let us look at the counts of weather situations by season.

```{r echo=FALSE}
# Compare weather situation counts for the four seasons.
seasonWeather <- table(bikeTrain$season, bikeTrain$weathersit)

knitr::kable(
  seasonWeather,
  col.names=c("Pleasant", "Overcast", "Unpleasant", "Severe"),
  caption="Table 2: Weather Situation Counts by Season"
)
```

We have examined the relationship between the seasons and weather, which we believe are important, but we want to predict the bike rentals given these features. So now we look at how our features' relationships with bike rentals.

Let's start with a [kernel density estimate plot](https://en.wikipedia.org/wiki/Kernel_density_estimation) of bike rentals and box plots of bike rentals by weather situation.

```{r}
###
# Make a KDE plot of bike rentals and box plots of bike rentals by weather 
# situation.
###

plot7 <- ggplot(bikeTrain, aes(x=cnt)) + 
  geom_density(bw=250, color="darkblue", fill="lightblue") + 
  xlab("Bike Rentals") + 
  ggtitle("KDE Plot of Bike Rentals")

plot8 <- ggplot(bikeTrain, aes(x=weathersit, y=cnt, 
                                        color=weathersit)) + 
  geom_boxplot() + 
  geom_jitter(alpha=0.25) + 
  xlab("Weather Situation") + 
  ylab("Bike Rentals") + 
  ggtitle("Bike Rentals by Weather") +
  theme(legend.position="none")

plot_grid(plot7, plot8)
```

Now let's examine the effects of temperature, humidity, wind speed, and season.

```{r}
###
# Create scatter plots of bike rentals vs. temp, hum, and windspeed with 
# regression lines and a box plots of bike rentals by season.
###

plot9 <- ggplot(bikeTrain, aes(x=temp*41, y=cnt)) + 
  geom_point(color="blue", alpha=0.5) + 
  geom_smooth(method = 'lm', formula='y ~ poly(x, 2)', color="red") +
  xlab("Temp. (Celsius)") + 
  ylab("Rentals") + 
  ggtitle("Bike Rentals vs. Temp.")

plot10 <- ggplot(bikeTrain, aes(x=hum*100, y=cnt)) + 
  geom_point(color="blue", alpha=0.5) + 
  geom_smooth(method = 'lm', formula='y ~ poly(x, 2)', color="red") +
  xlab("Humidity") + 
  ylab("Rentals") + 
  ggtitle("Bike Rentals vs. Humidity")

plot11 <- ggplot(bikeTrain, aes(x=windspeed*67, y=cnt)) + 
  geom_point(color="blue", alpha=0.5) + 
  geom_smooth(method = 'lm', formula='y ~ poly(x, 2)', color="red") +
  xlab("Wind Speed (km/h)") + 
  ylab("Rentals") + 
  ggtitle("Bike Rentals vs. Wind Speed")

plot12 <- ggplot(bikeTrain, aes(x=season, y=cnt, color=season)) + 
  geom_boxplot() + 
  geom_jitter(alpha=0.5) + 
  xlab("Season") + 
  ylab("Bike Rentals") + 
  ggtitle("Bike Rentals by Season") +
  theme(legend.position="none")

plot_grid(plot9, plot10, plot11, plot12)
```



# Modeling

## Linear model 

### 2 


```{r}
bikeTrain$season <- as.numeric(bikeTrain$season)
pairs(bikeTrain )
lmfit1a <- lm(casual~temp, data=bikeTrain)
summary(lmfit1a)

lmfit1b <- lm(casual~ mnth, data=bikeTrain)
summary(lmfit1b) 

lmfit1c <- lm(casual~ windspeed , data=bikeTrain)
summary(lmfit1c) 

lmfit1d <- lm(casual~season , data=bikeTrain)
summary(lmfit1d) 

lmfit2a <- lm(casual~temp*season, data=bikeTrain)
summary(lmfit2a)

lmfit2b <- lm(casual~temp*windspeed, data=bikeTrain)
summary(lmfit2b)

lmfit2c <- lm(casual~season*windspeed, data=bikeTrain)
summary(lmfit2c)

lmfit3a <- lm(casual~ temp + I(temp^2), data=bikeTrain)
summary(lmfit3a) 

lmfit3b <- lm(casual~ windspeed + I(windspeed ^2), data=bikeTrain)
summary(lmfit3b)
lmfit3c <- lm(casual~ season + I(season^2), data=bikeTrain)
summary(lmfit3c) 

lmfit4a <- lm(casual~temp*windspeed, data=bikeTrain)
summary(lmfit4a)

lmfit4b <- lm(casual~temp*season, data=bikeTrain)
summary(lmfit4b)

lmfit5a <- lm(casual~temp*windspeed*season, data=bikeTrain)
summary(lmfit5a)

lmfit5b <- lm(casual~temp*windspeed*season + I(temp^2)+ I(windspeed^2)+ I(season^2), data=bikeTrain)
summary(lmfit5b)

trainFit <- function (fit) {
  model <- round(c(summary(fit)$adj.r.squared, AIC(fit), 
                   MuMIn::AICc(fit), BIC(fit)), 3)
  name <- c("adj.r.sqr", "AIC","AICc", "BIC")
  df_stat <- data.frame(name, model)
  df_stat2 <- spread(df_stat, name, model)
}
stat1a <- trainFit(lmfit1a)
stat1b <- trainFit(lmfit1b)
stat1c <- trainFit(lmfit1c)
stat1d <- trainFit(lmfit1c)
  
stat2a <- trainFit(lmfit2a)
stat2b <- trainFit(lmfit2b)
stat2c <- trainFit(lmfit2c)

stat3a <- trainFit(lmfit3a)
stat3b <- trainFit(lmfit3b)
stat3c <- trainFit(lmfit3c)
    
stat4a <- trainFit(lmfit4a)        
stat4b <- trainFit(lmfit4b) 

stat5a<- trainFit(lmfit5a)
stat5b<- trainFit(lmfit5a)

stat_sum <- rbind(stat1a,stat1b,stat1c,stat1d,stat2a,stat2b,stat2c,stat3a,stat3b,stat3c,stat4a,stat4b,stat5a,stat5b)
rownames (stat_sum) <-c("lmfit1a","lmfit1b","lmfit1c","lmfit1d","lmfit2a","lmfit2b","lmfit2c", "lmfit3a","lmfit3b","lmfit3c","lmfit4a","lmfit4b","lmfit5a","lmfit5b") 
stat_sum 
```

## Ensemble tree model

Boosting is a way to slowly train a tree. 

### boosted tree 
```{r, echo=T, results='hide'}
set.seed(50)
gbmGrid <- expand.grid(interaction.depth=4, n.trees = 1000,
                   shrinkage=0.1,
                   n.minobsinnode=10)
gbmFit <- train (casual ~., data=select(bikeTrain, -"weekday"),
                   method="gbm",
                   preProcess= c("center", "scale"),
                   trControl = trainControl(method = "cv",
                                            number=10),
                    tuneGrid=gbmGrid)
gbmFit 
```

# Comparison  

```{r}

lmPred <- predict(lmfit1a,  newdata=bikeTest)
lmTest <- postResample(lmPred , bikeTest$casual)

lmPred <- predict(lmfit1a,  newdata=bikeTest)
lmTest <- postResample(lmPred , bikeTest$casual)

lmTest 
boostPred <- predict (gbmFit, newdata=select (bikeTest, - casual ), n.trees=1000)
boostTest <- postResample(boostPred, bikeTest$casual)
boostTest

#convert season variable of test data from character into digital factors for prediction
bikeTest$season <-  factor(ifelse(bikeTest$season ==  "Spring", 1,   
                                ifelse(bikeTest$season == "Summer", 2, 
                                       ifelse(bikeTest$season == "Fall",3, 
                                              4))))

#Predict on test data using optimized boosted tree model
boostPred <- predict (gbmFit, newdata=bikeTest )

#Get RMSE, Rsquared and MAE from predicted models for comparison
boostTest <- postResample (boostPred, bikeTest$casual)
boostTest

```


